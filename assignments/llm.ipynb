{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI & Machine Learning (KAN-CINTO4003U) - Copenhagen Business School | Spring 2025**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part III: LLM\n",
    "\n",
    "Please see the description of the assignment in the README file (section 3) <br>\n",
    "**Guide notebook**: [guides/llm_guide.ipynb](guides/llm_guide.ipynb)\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "* Note that you should report results using a classification report. \n",
    "\n",
    "* Also, remember to include some reflections on your results: how do they compare with the results from Part I, BoW?, and part II, BERT? Are there any hyperparameters or prompting techniques that are particularly important?\n",
    "\n",
    "* You should follow the steps given in the `llm_guide` notebook\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the project\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report \n",
    "from tqdm import tqdm\n",
    "from decouple import config\n",
    "from ibm_watsonx_ai import APIClient\n",
    "from ibm_watsonx_ai import Credentials\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.foundation_models.schema import TextGenParameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data\n",
    "\n",
    "We can load this data directly from [Hugging Face Datasets](https://huggingface.co/docs/datasets/) - The HuggingFace Hub- into a Pandas DataFrame. Pretty neat!\n",
    "\n",
    "**Note**: This cell will download the dataset and keep it in memory. If you run this cell multiple times, it will download the dataset multiple times.\n",
    "\n",
    "You are welcome to increase the `frac` parameter to load more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "# train = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"train\"])\n",
    "test = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(304, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {\n",
    "    0: 'World',\n",
    "    1: 'Sports',\n",
    "    2: 'Business',\n",
    "    3: 'Sci/Tech'\n",
    "}\n",
    "\n",
    "def preprocess(df: pd.DataFrame, frac = 2e-2, label_map = label_map, seed=42) -> pd.DataFrame:\n",
    "    return  (\n",
    "        df\n",
    "        .assign(label=lambda x: x['label'].map(label_map))\n",
    "        [lambda df: df['label'].isin(label_map.values())]\n",
    "        .groupby('label')\n",
    "        .apply(lambda x: x.sample(frac=frac, random_state=seed))\n",
    "        .reset_index(drop=True)\n",
    "\n",
    "    )\n",
    "\n",
    "# train_df = preprocess(train, frac=0.01)\n",
    "test_df = preprocess(test, frac=0.04)\n",
    "\n",
    "# clear up some memory by deleting the original dataframes\n",
    "# del train\n",
    "del test\n",
    "\n",
    "test_df.shape #, train_df.shape, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Establish connection and set model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables using python-decouple\n",
    "\n",
    "WX_API_KEY = config('WX_API_KEY')\n",
    "\n",
    "credentials = Credentials(\n",
    "    url = \"https://us-south.ml.cloud.ibm.com\",\n",
    "    api_key = WX_API_KEY\n",
    ")\n",
    "\n",
    "client = APIClient(\n",
    "    credentials=credentials, \n",
    "    project_id=\"32b6a744-5b49-4e5b-aee3-d9c56284fecf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = TextGenParameters(\n",
    "    temperature=0,              # Higher temperature means more randomness - In this case we don't want randomness\n",
    "    max_new_tokens=70,          # allow more room for CoT to reason\n",
    "    stop_sequences=[\".\", \"\\n\"], # Stop generating text when these sequences are encountered\n",
    ")\n",
    "\n",
    "model = ModelInference(\n",
    "    api_client=client,\n",
    "    model_id=\"ibm/granite-13b-instruct-v2\",\n",
    "    params=PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a system prompt with basic, fewshot, and chain-of-thought prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You task is to classify news stories into one of four categories — exactly as written below.\n",
    "\n",
    "CATEGORIES:\n",
    "{categories}\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "Please assign the correct category to the text. Answer with the correct category and nothing else.\n",
    "\n",
    "Category:\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT_FEWSHOT = \"\"\"You are a news classifier. Your task is to assign one of the following four CATEGORIES to a news story - only use the four categories listed below, nothing else!!!\n",
    "\n",
    "CATEGORIES:\n",
    "{categories}\n",
    "\n",
    "Examples:\n",
    "Example 1:\n",
    "TEXT: \"The United Nations convened today to discuss global strategies on climate change.\"\n",
    "Category: World\n",
    "\n",
    "Example 2:\n",
    "TEXT: \"The local team clinched the championship title in a stunning overtime finish.\"\n",
    "Category: Sports\n",
    "\n",
    "Example 3:\n",
    "TEXT: \"The stock market rallied today after a series of positive economic reports.\"\n",
    "Category: Business\n",
    "\n",
    "Example 4:\n",
    "TEXT: \"Scientists have unveiled a groundbreaking discovery in renewable energy technology.\"\n",
    "Category: Sci/Tech\n",
    "\n",
    "Now, classify the following news text:\n",
    "\n",
    "TEXT: {text}\n",
    "\n",
    "Answer with the correct category and nothing else (you can choose between the four categories I gave you).\n",
    "\n",
    "Category:\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT_CHAIN = \"\"\"You are a news classifier. Analyze the following news text and determine the most appropriate category from the list below — exactly as written.\n",
    "\n",
    "CATEGORIES:\n",
    "{categories}\n",
    "\n",
    "Instructions:\n",
    "1. Briefly think through the key points in the text.\n",
    "2. Identify important keywords or themes.\n",
    "3. Decide which category best fits the text.\n",
    "4. Output only a single line starting with 'Final Answer:' followed by one of the following exact labels: World, Sports, Business, Sci/Tech. Do not include any additional text.\n",
    "\n",
    "TEXT: {text}\n",
    "\n",
    "Let's think step-by-step internally. \n",
    "\n",
    "Final Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing baseline prompt: 100%|██████████| 304/304 [01:38<00:00,  3.08it/s]\n",
      "Processing few_shot prompt: 100%|██████████| 304/304 [01:36<00:00,  3.16it/s]\n",
      "Processing chain_of_thought prompt: 100%|██████████| 304/304 [01:34<00:00,  3.22it/s]\n"
     ]
    }
   ],
   "source": [
    "CATEGORIES = \"- \" + \"\\n- \".join(test_df[\"label\"].unique())  # Create a string with all categories\n",
    "\n",
    "# Create a dictionary of prompt variants\n",
    "prompts = {\n",
    "    \"baseline\": SYSTEM_PROMPT,\n",
    "    \"few_shot\": SYSTEM_PROMPT_FEWSHOT,\n",
    "    \"chain_of_thought\": SYSTEM_PROMPT_CHAIN\n",
    "}\n",
    "\n",
    "\n",
    "# Dictionary to hold the predictions for each prompt type\n",
    "results = {}\n",
    "\n",
    "for prompt_name, prompt_template in prompts.items():\n",
    "    predictions = []\n",
    "    # Loop over the test texts with a progress bar indicating the prompt variant being processed\n",
    "    for text in tqdm(test_df[\"text\"], desc=f\"Processing {prompt_name} prompt\"):\n",
    "        # Format the current prompt with the test text\n",
    "        prompt = prompt_template.format(categories=CATEGORIES, text=text)\n",
    "        \n",
    "        # Generate the response from the model\n",
    "        response = model.generate(prompt)\n",
    "        \n",
    "        # Extract the generated text and strip any extra whitespace\n",
    "        prediction = response[\"results\"][0][\"generated_text\"].strip()\n",
    "        \n",
    "        # Append the prediction to the list\n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    # Store predictions for the current prompt type\n",
    "    results[prompt_name] = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for prompt variant: baseline\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Business       0.54      0.95      0.69        76\n",
      "    Sci/Tech       0.90      0.34      0.50        76\n",
      "      Sports       0.93      0.91      0.92        76\n",
      "       World       0.84      0.75      0.79        76\n",
      "\n",
      "    accuracy                           0.74       304\n",
      "   macro avg       0.80      0.74      0.72       304\n",
      "weighted avg       0.80      0.74      0.72       304\n",
      "\n",
      "Evaluation for prompt variant: few_shot\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Business       0.47      0.95      0.63        76\n",
      "    Sci/Tech       0.90      0.34      0.50        76\n",
      "      Sports       0.91      0.89      0.90        76\n",
      "       World       0.81      0.50      0.62        76\n",
      "\n",
      "    accuracy                           0.67       304\n",
      "   macro avg       0.77      0.67      0.66       304\n",
      "weighted avg       0.77      0.67      0.66       304\n",
      "\n",
      "Evaluation for prompt variant: chain_of_thought\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Business       0.52      0.93      0.67        76\n",
      "    Sci/Tech       0.86      0.41      0.55        76\n",
      "      Sports       0.94      0.83      0.88        76\n",
      "       World       0.83      0.70      0.76        76\n",
      "\n",
      "    accuracy                           0.72       304\n",
      "   macro avg       0.79      0.72      0.71       304\n",
      "weighted avg       0.79      0.72      0.71       304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate each set of predictions using classification_report\n",
    "for prompt_variant, preds in results.items():\n",
    "    print(f\"Evaluation for prompt variant: {prompt_variant}\")\n",
    "    print(classification_report(test_df[\"label\"], preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection on Hyperparameters and Performance\n",
    "\n",
    "In this final part of the project, I implemented an LLM-based text classification system using IBM’s `granite-13b-instruct-v2` model via the watsonx platform. The system was tasked with assigning AG News samples to one of four categories using **prompt engineering**, without any model fine-tuning.\n",
    "\n",
    "I experimented with several prompting strategies:\n",
    "- **Baseline** (zero-shot classification with structured instructions)\n",
    "- **Few-shot** (adding 1 example per class)\n",
    "- **Chain-of-Thought (CoT)** (step-by-step reasoning before choosing a label)\n",
    "\n",
    "\n",
    "Across all techniques, the baseline zero-shot prompt consistently achieved the highest accuracy (~74%) and macro F1-score (~0.72). Neither few-shot nor chain-of-thought prompting improved the performance. In fact, both led to slightly **lower accuracy and consistency** across classes.\n",
    "\n",
    "I also adjusted parameters like `max_new_tokens`, but I was not able to surpass the baseline performance. The Sci/Tech category in particular was difficult for all techniques, which often received poor recall regardless of prompting strategy.\n",
    "\n",
    "Unfortunately, despite these efforts, **performance did not improve**, and I am unsure of the specific reasons. This may relate to:\n",
    "- Limits of zero-shot/few-shot prompting for classification\n",
    "- The model's sensitivity to prompt phrasing or structure\n",
    "- Difficulty in classifying short, context-light texts with LLMs without tuning\n",
    "\n",
    "### Comparison with BoW & BERT models\n",
    "| Model       | Accuracy | Macro F1 | Key Takeaway |\n",
    "|-------------|----------|----------|---------------|\n",
    "| **BoW + LogReg** | ~82%     | ~82%     | Simple but effective baseline |\n",
    "| **BERT + LogReg** | ~88%     | ~88%     | Most robust and balanced overall |\n",
    "| **LLM (Prompted)** | ~74%     | ~72%     | Good zero-shot, but no improvement from advanced prompting |\n",
    "\n",
    "Compared to both traditional BoW and BERT-based classifiers, the LLM approach **underperformed**. The BERT-based model, in particular, offered the best trade-off between performance and effort. It outperformed the LLM without any prompting tricks or large token budgets.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml25-ma2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
