{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI & Machine Learning (KAN-CINTO4003U) - Copenhagen Business School | Spring 2025**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part III: LLM\n",
    "\n",
    "Please see the description of the assignment in the README file (section 3) <br>\n",
    "**Guide notebook**: [guides/llm_guide.ipynb](guides/llm_guide.ipynb)\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "* Note that you should report results using a classification report. \n",
    "\n",
    "* Also, remember to include some reflections on your results: how do they compare with the results from Part I, BoW?, and part II, BERT? Are there any hyperparameters or prompting techniques that are particularly important?\n",
    "\n",
    "* You should follow the steps given in the `llm_guide` notebook\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the project\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report \n",
    "from tqdm import tqdm\n",
    "from decouple import config\n",
    "from ibm_watsonx_ai import APIClient\n",
    "from ibm_watsonx_ai import Credentials\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.foundation_models.schema import TextGenParameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data\n",
    "\n",
    "We can load this data directly from [Hugging Face Datasets](https://huggingface.co/docs/datasets/) - The HuggingFace Hub- into a Pandas DataFrame. Pretty neat!\n",
    "\n",
    "**Note**: This cell will download the dataset and keep it in memory. If you run this cell multiple times, it will download the dataset multiple times.\n",
    "\n",
    "You are welcome to increase the `frac` parameter to load more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 9cbd8221-0f88-42bf-8f3f-3af0ea00b1c9)')' thrown while requesting GET https://huggingface.co/datasets/fancyzhx/ag_news/resolve/main/data/test-00000-of-00001.parquet\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    }
   ],
   "source": [
    "\n",
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "# train = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"train\"])\n",
    "test = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {\n",
    "    0: 'World',\n",
    "    1: 'Sports',\n",
    "    2: 'Business',\n",
    "    3: 'Sci/Tech'\n",
    "}\n",
    "\n",
    "def preprocess(df: pd.DataFrame, frac = 1e-2, label_map = label_map, seed=42) -> pd.DataFrame:\n",
    "    return  (\n",
    "        df\n",
    "        .assign(label=lambda x: x['label'].map(label_map))\n",
    "        [lambda df: df['label'].isin(label_map.values())]\n",
    "        .groupby('label')\n",
    "        .apply(lambda x: x.sample(frac=frac, random_state=seed))\n",
    "        .reset_index(drop=True)\n",
    "\n",
    "    )\n",
    "\n",
    "# train_df = preprocess(train, frac=0.01)\n",
    "test_df = preprocess(test, frac=0.01)\n",
    "\n",
    "# clear up some memory by deleting the original dataframes\n",
    "# del train\n",
    "del test\n",
    "\n",
    "test_df.shape #, train_df.shape, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Establish connection and set model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables using python-decouple\n",
    "\n",
    "WX_API_KEY = config('WX_API_KEY')\n",
    "\n",
    "credentials = Credentials(\n",
    "    url = \"https://us-south.ml.cloud.ibm.com\",\n",
    "    api_key = WX_API_KEY\n",
    ")\n",
    "\n",
    "client = APIClient(\n",
    "    credentials=credentials, \n",
    "    project_id=\"32b6a744-5b49-4e5b-aee3-d9c56284fecf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = TextGenParameters(\n",
    "    temperature=0,              # Higher temperature means more randomness - In this case we don't want randomness\n",
    "    max_new_tokens=10,          # Maximum number of tokens to generate\n",
    "    stop_sequences=[\".\", \"\\n\"], # Stop generating text when these sequences are encountered\n",
    ")\n",
    "\n",
    "model = ModelInference(\n",
    "    api_client=client,\n",
    "    model_id=\"ibm/granite-13b-instruct-v2\",  # We could also try a larger model!\n",
    "    params=PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a system prompt with basic, fewshot, and chain-of-thought prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You task is to classify news stories into one of five categories\n",
    "\n",
    "CATEGORIES:\n",
    "{categories}\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "Please assign the correct category to the text. Answer with the correct category and nothing else.\n",
    "\n",
    "Category:\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT_FEWSHOT = \"\"\"You are a news classifier. Your task is to assign one of the following categories to a news story.\n",
    "\n",
    "CATEGORIES:\n",
    "{categories}\n",
    "\n",
    "Examples:\n",
    "Example 1:\n",
    "TEXT: \"The United Nations convened today to discuss global strategies on climate change.\"\n",
    "Category: World\n",
    "\n",
    "Example 2:\n",
    "TEXT: \"The local team clinched the championship title in a stunning overtime finish.\"\n",
    "Category: Sports\n",
    "\n",
    "Now, classify the following news text:\n",
    "\n",
    "TEXT: {text}\n",
    "\n",
    "Answer with the correct category and nothing else.\n",
    "\n",
    "Category:\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT_CHAIN = \"\"\"You are a news classifier. Your job is to analyze the news text and determine the most appropriate category from the list provided.\n",
    "\n",
    "CATEGORIES:\n",
    "{categories}\n",
    "\n",
    "Process:\n",
    "1. Briefly summarize the key points in the text.\n",
    "2. Identify important keywords or themes.\n",
    "3. Decide which category best fits the text.\n",
    "4. Provide only the final category as the answer. Answer with the correct category and nothing else.\n",
    "\n",
    "TEXT: {text}\n",
    "\n",
    "Let's think step-by-step:\n",
    "1.\n",
    "2.\n",
    "3.\n",
    "Final Category:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing baseline prompt: 100%|██████████| 76/76 [00:29<00:00,  2.57it/s]\n",
      "Processing few_shot prompt: 100%|██████████| 76/76 [00:28<00:00,  2.71it/s]\n",
      "Processing chain_of_thought prompt: 100%|██████████| 76/76 [00:30<00:00,  2.51it/s]\n"
     ]
    }
   ],
   "source": [
    "CATEGORIES = \"- \" + \"\\n- \".join(test_df[\"label\"].unique())  # Create a string with all categories\n",
    "\n",
    "# Create a dictionary of prompt variants\n",
    "prompts = {\n",
    "    \"baseline\": SYSTEM_PROMPT,\n",
    "    \"few_shot\": SYSTEM_PROMPT_FEWSHOT,\n",
    "    \"chain_of_thought\": SYSTEM_PROMPT_CHAIN\n",
    "}\n",
    "\n",
    "\n",
    "# Dictionary to hold the predictions for each prompt type\n",
    "results = {}\n",
    "\n",
    "for prompt_name, prompt_template in prompts.items():\n",
    "    predictions = []\n",
    "    # Loop over the test texts with a progress bar indicating the prompt variant being processed\n",
    "    for text in tqdm(test_df[\"text\"], desc=f\"Processing {prompt_name} prompt\"):\n",
    "        # Format the current prompt with the test text\n",
    "        prompt = prompt_template.format(categories=CATEGORIES, text=text)\n",
    "        \n",
    "        # Generate the response from the model\n",
    "        response = model.generate(prompt)\n",
    "        \n",
    "        # Extract the generated text and strip any extra whitespace\n",
    "        prediction = response[\"results\"][0][\"generated_text\"].strip()\n",
    "        \n",
    "        # Append the prediction to the list\n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    # Store predictions for the current prompt type\n",
    "    results[prompt_name] = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for prompt variant: baseline\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Business       0.55      0.95      0.69        19\n",
      "    Sci/Tech       1.00      0.26      0.42        19\n",
      "      Sports       1.00      0.89      0.94        19\n",
      "       World       0.76      0.84      0.80        19\n",
      "\n",
      "    accuracy                           0.74        76\n",
      "   macro avg       0.83      0.74      0.71        76\n",
      "weighted avg       0.83      0.74      0.71        76\n",
      "\n",
      "Evaluation for prompt variant: few_shot\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Business       0.50      0.95      0.65        19\n",
      "    Sci/Tech       0.89      0.42      0.57        19\n",
      "      Sports       1.00      0.84      0.91        19\n",
      "       World       0.87      0.68      0.76        19\n",
      "\n",
      "    accuracy                           0.72        76\n",
      "   macro avg       0.81      0.72      0.73        76\n",
      "weighted avg       0.81      0.72      0.73        76\n",
      "\n",
      "Evaluation for prompt variant: chain_of_thought\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Business       0.51      0.95      0.67        19\n",
      "    Politics       0.00      0.00      0.00         0\n",
      "    Sci/Tech       0.89      0.42      0.57        19\n",
      "      Sports       0.84      0.84      0.84        19\n",
      "       World       0.82      0.47      0.60        19\n",
      "\n",
      "    accuracy                           0.67        76\n",
      "   macro avg       0.61      0.54      0.54        76\n",
      "weighted avg       0.77      0.67      0.67        76\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/claud/anaconda3/envs/aiml25-ma2/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/claud/anaconda3/envs/aiml25-ma2/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/claud/anaconda3/envs/aiml25-ma2/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate each set of predictions using classification_report\n",
    "for prompt_variant, preds in results.items():\n",
    "    print(f\"Evaluation for prompt variant: {prompt_variant}\")\n",
    "    print(classification_report(test_df[\"label\"], preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml25-ma2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
