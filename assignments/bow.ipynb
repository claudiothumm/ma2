{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI & Machine Learning (KAN-CINTO4003U) - Copenhagen Business School | Spring 2025**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part I: Bag-of-Words Model\n",
    "\n",
    "Please see the description of the assignment in the README file (section 1) <br>\n",
    "**Guide notebook**: [guides/bow_guide.ipynb](guides/bow_guide.ipynb)\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "* Note that you should report results using a classification report. \n",
    "\n",
    "* Also, remember to include some reflections on your results: Are there any hyperparameters that are particularly important?\n",
    "\n",
    "* You should follow the steps given in the `bow_guide` notebook\n",
    "\n",
    "<br>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the project\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data\n",
    "\n",
    "We can load this data directly from [Hugging Face Datasets](https://huggingface.co/docs/datasets/) - The HuggingFace Hub- into a Pandas DataFrame. Pretty neat!\n",
    "\n",
    "**Note**: This cell will download the dataset and keep it in memory. If you run this cell multiple times, it will download the dataset multiple times.\n",
    "\n",
    "You are welcome to increase the `frac` parameter to load more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 2) (7600, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "\n",
    "train = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"train\"])\n",
    "test = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"test\"])\n",
    "\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1200, 2), (760, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "label_map = {\n",
    "    0: 'World',\n",
    "    1: 'Sports',\n",
    "    2: 'Business',\n",
    "    3: 'Sci/Tech'\n",
    "}\n",
    "\n",
    "def preprocess(df: pd.DataFrame, frac : float = 1e-2, label_map : dict[int, str] = label_map, seed : int = 42) -> pd.DataFrame:\n",
    "    \"\"\" Preprocess the dataset \n",
    "\n",
    "    Operations:\n",
    "    - Map the label to the corresponding category\n",
    "    - Filter out the labels not in the label_map\n",
    "    - Sample a fraction of the dataset (stratified by label)\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): The dataset to preprocess\n",
    "    - frac (float): The fraction of the dataset to sample in each category\n",
    "    - label_map (dict): A mapping of the original label to the new label\n",
    "    - seed (int): The random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The preprocessed dataset\n",
    "    \"\"\"\n",
    "\n",
    "    return  (\n",
    "        df\n",
    "        .assign(label=lambda x: x['label'].map(label_map))\n",
    "        [lambda df: df['label'].isin(label_map.values())]\n",
    "        .groupby('label')[[\"text\", \"label\"]]\n",
    "        .apply(lambda x: x.sample(frac=frac, random_state=seed))\n",
    "        .reset_index(drop=True)\n",
    "\n",
    "    )\n",
    "\n",
    "train_df = preprocess(train, frac=0.01)\n",
    "test_df = preprocess(test, frac=0.1)\n",
    "\n",
    "# clear up some memory by deleting the original dataframes\n",
    "del train\n",
    "del test\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(960,) (240,) (960,) (240,)\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    \n",
    "    X_train,\n",
    "    X_val,\n",
    "    y_train,\n",
    "    y_val\n",
    "\n",
    ") = train_test_split(train_df[\"text\"], train_df[\"label\"], test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tfidfvectorizer\n",
    "tv = TfidfVectorizer()\n",
    "# fit_transform on the training data\n",
    "X_train_vectorized = tv.fit_transform(X_train)\n",
    "# get dense matrix\n",
    "X_train_vectorized.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 10, 'class_weight': None, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Best score: 0.823820572389945\n"
     ]
    }
   ],
   "source": [
    "# Insert parameters (via GridSearch) and reflect on them\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10], # Regularization strength: lower values = stronger regularization to prevent overfitting\n",
    "    'penalty': ['l1', 'l2'], # Test both L1 (sparse features) and L2 (smooth weights) regularization\n",
    "    'solver': ['liblinear'], # 'liblinear' supports both 'l1' and 'l2' and works well for small to medium-sized datasets\n",
    "    'class_weight': [None, 'balanced'] # Try both: 'balanced' adjusts for class imbalance; None leaves classes unweighted\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    LogisticRegression(max_iter=1000), # Increase iterations to ensure convergence, especially with small C values\n",
    "    param_grid,\n",
    "    cv=5, # 5-fold cross-validation for more stable and reliable performance estimates\n",
    "    scoring='f1_macro', # Use macro-averaged F1 to treat all classes equally, regardless of support\n",
    "    n_jobs=-1 # Use all available CPU cores for faster grid search\n",
    ")\n",
    "\n",
    "grid.fit(X_train_vectorized, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(\"Best score:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predicitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid.best_estimator_\n",
    "\n",
    "X_val_vectorized = tv.transform(X_val)  # transform, not fit_transform (for the validation data)\n",
    "y_pred = best_model.predict(X_val_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on the training set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       World       1.00      1.00      1.00       238\n",
      "      Sports       1.00      1.00      1.00       240\n",
      "    Business       1.00      1.00      1.00       240\n",
      "    Sci/Tech       1.00      1.00      1.00       242\n",
      "\n",
      "    accuracy                           1.00       960\n",
      "   macro avg       1.00      1.00      1.00       960\n",
      "weighted avg       1.00      1.00      1.00       960\n",
      "\n",
      "Performance on the validation set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       World       0.82      0.73      0.77        62\n",
      "      Sports       0.77      0.67      0.71        60\n",
      "    Business       0.81      0.93      0.87        60\n",
      "    Sci/Tech       0.80      0.88      0.84        58\n",
      "\n",
      "    accuracy                           0.80       240\n",
      "   macro avg       0.80      0.80      0.80       240\n",
      "weighted avg       0.80      0.80      0.80       240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance on the training set:\")\n",
    "print(classification_report(y_train, best_model.predict(X_train_vectorized), target_names=label_map.values()))\n",
    "\n",
    "print(\"Performance on the validation set:\")\n",
    "print(classification_report(y_val, y_pred, target_names=label_map.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on the test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       World       0.74      0.79      0.77       190\n",
      "      Sports       0.83      0.76      0.79       190\n",
      "    Business       0.88      0.91      0.89       190\n",
      "    Sci/Tech       0.86      0.84      0.85       190\n",
      "\n",
      "    accuracy                           0.82       760\n",
      "   macro avg       0.83      0.83      0.82       760\n",
      "weighted avg       0.83      0.82      0.82       760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df_vectorized = tv.transform(test_df[\"text\"])\n",
    "\n",
    "print(\"Performance on the test set:\")\n",
    "print(classification_report(test_df[\"label\"], best_model.predict(test_df_vectorized), target_names=label_map.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection on Hyperparameters and Performance\n",
    "\n",
    "The final model demonstrates solid performance on both the validation and test sets. After performing hyperparameter tuning to find the best hyperparameters with `GridSearchCV` and switching from `CountVectorizer` to `TfidfVectorizer`, the results improved.\n",
    "\n",
    "#### Performance summary\n",
    "- **Training accuracy:** 100%\n",
    "- **Validation accuracy:** 80%\n",
    "- **Test accuracy:** 82%\n",
    "- **Best macro F1 (cross-validation):** 0.82\n",
    "\n",
    "On the test set:\n",
    "- **Business** and **Sci/Tech** classes perform quite well (F1: 0.89 and 0.85 respectively).\n",
    "- **World** and **Sports** also achieve relatively good scores (F1: 0.77 and 0.79).\n",
    "\n",
    "The grid search revealed the following configuration for `LogisticRegression`:\n",
    "- `C=10` — relatively low regularization, which allows the model to learn more nuanced weights.\n",
    "- `penalty='l2'` — standard L2 regularization helped maintain generalization without over-sparsifying.\n",
    "- `solver='liblinear'` — suitable for small datasets and compatible with L1/L2 penalties.\n",
    "- `class_weight=None` — the model performed best without adjusting class weights, which suggests that the class imbalance was not a major issue.\n",
    "\n",
    "\n",
    "#### Final thoughts\n",
    "The model achieved perfect scores on the training set — 100% precision, recall, and F1-score across all categories. While this might initially seem good, it’s typically a **strong indicator of overfitting**.\n",
    "Even though the training performance was perfect, the **validation and test performance remained strong**, which shows that the model **generalized well despite overfitting on training data**. Through the utilization of the `TfidfVectorizer` and the `GridSearchCV`, the performance could be improved even more. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml25-ma2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
